{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import soundfile\n",
    "import os, glob, pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from playsound import playsound \n",
    "import pyaudio\n",
    "import wave \n",
    "import pickle ##for storing the model once created in pkl extension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleansing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "##for data cleansing, we lower down the sampling rate and ofcourse try to cut down the noise in our audio file\n",
    "def clean(y,rate,threshold):\n",
    "    mask=[]\n",
    "    y = pd.Series(y).apply(np.abs) #we use rolling window analysis for time-series data \n",
    "    y_mean= y.rolling(window = int(rate / 10),min_periods = 1, center = True).mean()\n",
    "    #each  window is size of frq./ 10 and we check if mean > threshold, we deem it true, else its noise we deem it false.\n",
    "    for mean in y_mean:\n",
    "        if mean > threshold:\n",
    "            mask.append(True)\n",
    "        else:\n",
    "            mask.append(False)\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Store the Cleansed Audio in Clean Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "##storing clean data in new directory using wavfile from scipy\n",
    "from scipy.io import wavfile\n",
    "for file in glob.glob(\"E:\\\\ravdess_data\\\\Actor_*\\\\*.wav\"):\n",
    "    file_name = os.path.basename(file)\n",
    "    signal , rate = librosa.load(file, sr=16000)\n",
    "    mask = clean(signal,rate, 0.0005)\n",
    "    wavfile.write(filename= r'E:\\\\clean_data\\\\clean_data'+str(file_name), rate=rate,data=signal[mask])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#works.\n",
    "#extracting all our features of mfcc and mel.\n",
    "#extract feature transpose if feature present from librosa, and extract the mean value of it.\n",
    "\n",
    "def extract_feature(file_name, mfcc, chroma, mel):\n",
    "    with soundfile.SoundFile(file_name) as sound_file:\n",
    "        X = sound_file.read(dtype=\"float32\") #convert the sound file to float32 type for our timeseries data\n",
    "        sample_rate=sound_file.samplerate ##process at the same samplerate as that of param provided audio\n",
    "        if chroma: #extracting short time fourier transform(stft): time freq values over windows. expected op a matrix.\n",
    "            stft=np.abs(librosa.stft(X))\n",
    "        result=np.array([])\n",
    "        if mfcc:\n",
    "            mfccs=np.mean(librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=40).T, axis=0)\n",
    "            ##y expects the time-series data of audio , sr is sample rate nd n_mfcc are the number of mfcc to return\n",
    "            result=np.hstack((result, mfccs)) #horizontal columnar store.\n",
    "        if chroma:\n",
    "            chroma=np.mean(librosa.feature.chroma_stft(S=stft, sr=sample_rate).T,axis=0) #expects abs(TimeSeriesdata)\n",
    "            result=np.hstack((result, chroma))\n",
    "        if mel:\n",
    "            mel=np.mean(librosa.feature.melspectrogram(X, sr=sample_rate).T,axis=0)\n",
    "            result=np.hstack((result, mel))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Label Classification & data-loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotions={\n",
    "  '01':'neutral',\n",
    "  '02':'calm',\n",
    "  '03':'happy',\n",
    "  '04':'sad',\n",
    "  '05':'angry',\n",
    "  '06':'fearful',\n",
    "  '07':'disgust',\n",
    "  '08':'surprised'\n",
    "}\n",
    "\n",
    "observed_emotions=['calm', 'happy', 'neutral', 'sad']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(test_size=0.33): ##33% test data \n",
    "    x,y=[],[]\n",
    "    answer = 0\n",
    "    for file in glob.glob(\"E:\\\\clean_data\\\\clean_data*.wav\"):\n",
    "        file_name=os.path.basename(file)\n",
    "        emotion=emotions[file_name.split(\"-\")[2]]\n",
    "        if emotion not in observed_emotions:\n",
    "            answer += 1\n",
    "            continue\n",
    "        feature=extract_feature(file, mfcc=True, chroma=True, mel=True)\n",
    "        x.append(feature)\n",
    "        y.append([emotion,file_name]) \n",
    "    return train_test_split(np.array(x), y, test_size=test_size, random_state=9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mapping Data Testing Data corresponding fileName Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(504, 180) (168, 180) (504, 2) (168, 2)\n",
      "(504,) (168,)\n",
      "clean_data03-01-02-01-01-02-22.wav\n",
      "clean_data03-01-04-01-02-02-23.wav\n",
      "clean_data03-01-04-02-01-02-05.wav\n",
      "clean_data03-01-03-01-01-02-11.wav\n",
      "clean_data03-01-03-02-02-02-16.wav\n",
      "clean_data03-01-02-02-02-02-21.wav\n",
      "clean_data03-01-01-01-02-02-20.wav\n",
      "clean_data03-01-03-02-01-02-03.wav\n",
      "clean_data03-01-02-02-02-01-11.wav\n",
      "clean_data03-01-01-01-02-01-15.wav\n",
      "clean_data03-01-01-01-01-02-21.wav\n",
      "clean_data03-01-02-01-01-01-21.wav\n",
      "clean_data03-01-03-02-02-01-16.wav\n",
      "clean_data03-01-02-01-02-01-03.wav\n",
      "clean_data03-01-03-01-02-01-14.wav\n",
      "clean_data03-01-03-02-01-01-06.wav\n",
      "clean_data03-01-02-01-02-02-06.wav\n",
      "clean_data03-01-03-02-02-02-23.wav\n",
      "clean_data03-01-02-02-01-01-01.wav\n",
      "clean_data03-01-04-01-01-01-03.wav\n",
      "clean_data03-01-04-01-02-02-13.wav\n",
      "clean_data03-01-01-01-01-02-17.wav\n",
      "clean_data03-01-02-02-02-02-08.wav\n",
      "clean_data03-01-02-01-01-01-11.wav\n",
      "clean_data03-01-01-01-01-02-16.wav\n",
      "clean_data03-01-02-02-01-01-04.wav\n",
      "clean_data03-01-04-02-02-02-24.wav\n",
      "clean_data03-01-03-02-01-02-11.wav\n",
      "clean_data03-01-01-01-01-02-07.wav\n",
      "clean_data03-01-03-01-01-02-18.wav\n",
      "clean_data03-01-02-02-02-02-20.wav\n",
      "clean_data03-01-01-01-02-02-18.wav\n",
      "clean_data03-01-04-01-01-02-02.wav\n",
      "clean_data03-01-03-01-02-02-24.wav\n",
      "clean_data03-01-04-02-02-02-16.wav\n",
      "clean_data03-01-04-01-01-01-12.wav\n",
      "clean_data03-01-01-01-02-01-08.wav\n",
      "clean_data03-01-04-02-01-02-01.wav\n",
      "clean_data03-01-01-01-02-01-13.wav\n",
      "clean_data03-01-04-01-01-02-17.wav\n",
      "clean_data03-01-01-01-02-02-23.wav\n",
      "clean_data03-01-02-01-02-02-22.wav\n",
      "clean_data03-01-03-01-01-01-02.wav\n",
      "clean_data03-01-04-02-01-02-02.wav\n",
      "clean_data03-01-02-01-02-02-11.wav\n",
      "clean_data03-01-02-02-01-02-16.wav\n",
      "clean_data03-01-04-02-01-01-04.wav\n",
      "clean_data03-01-03-02-01-01-14.wav\n",
      "clean_data03-01-04-02-02-02-08.wav\n",
      "clean_data03-01-02-01-01-01-04.wav\n",
      "clean_data03-01-04-02-02-02-21.wav\n",
      "clean_data03-01-02-02-02-01-20.wav\n",
      "clean_data03-01-03-01-01-02-05.wav\n",
      "clean_data03-01-02-01-01-01-17.wav\n",
      "clean_data03-01-03-02-01-02-24.wav\n",
      "clean_data03-01-04-01-02-01-19.wav\n",
      "clean_data03-01-04-02-02-01-19.wav\n",
      "clean_data03-01-03-02-02-02-21.wav\n",
      "clean_data03-01-04-02-01-01-15.wav\n",
      "clean_data03-01-01-01-01-02-24.wav\n",
      "clean_data03-01-03-01-02-01-24.wav\n",
      "clean_data03-01-01-01-02-01-21.wav\n",
      "clean_data03-01-01-01-02-02-06.wav\n",
      "clean_data03-01-03-01-01-01-05.wav\n",
      "clean_data03-01-04-02-01-01-24.wav\n",
      "clean_data03-01-02-01-01-01-02.wav\n",
      "clean_data03-01-02-02-02-02-05.wav\n",
      "clean_data03-01-02-02-02-01-21.wav\n",
      "clean_data03-01-04-01-01-02-13.wav\n",
      "clean_data03-01-03-02-02-01-06.wav\n",
      "clean_data03-01-04-01-01-01-09.wav\n",
      "clean_data03-01-02-02-02-02-10.wav\n",
      "clean_data03-01-03-01-02-02-05.wav\n",
      "clean_data03-01-02-02-02-02-16.wav\n",
      "clean_data03-01-03-02-01-02-16.wav\n",
      "clean_data03-01-04-01-01-01-05.wav\n",
      "clean_data03-01-02-02-02-01-18.wav\n",
      "clean_data03-01-03-02-01-01-11.wav\n",
      "clean_data03-01-01-01-01-02-09.wav\n",
      "clean_data03-01-03-01-01-02-19.wav\n",
      "clean_data03-01-03-01-02-02-03.wav\n",
      "clean_data03-01-01-01-02-01-24.wav\n",
      "clean_data03-01-03-02-02-02-03.wav\n",
      "clean_data03-01-01-01-01-01-05.wav\n",
      "clean_data03-01-01-01-01-01-03.wav\n",
      "clean_data03-01-03-01-02-01-07.wav\n",
      "clean_data03-01-04-02-02-02-22.wav\n",
      "clean_data03-01-02-02-01-02-12.wav\n",
      "clean_data03-01-04-01-02-01-04.wav\n",
      "clean_data03-01-04-01-01-01-17.wav\n",
      "clean_data03-01-03-02-02-01-11.wav\n",
      "clean_data03-01-02-02-02-01-15.wav\n",
      "clean_data03-01-04-02-02-01-16.wav\n",
      "clean_data03-01-01-01-02-02-01.wav\n",
      "clean_data03-01-03-02-01-02-14.wav\n",
      "clean_data03-01-02-01-02-02-17.wav\n",
      "clean_data03-01-03-02-02-02-24.wav\n",
      "clean_data03-01-02-01-02-01-16.wav\n",
      "clean_data03-01-01-01-02-01-07.wav\n",
      "clean_data03-01-03-01-02-01-10.wav\n",
      "clean_data03-01-02-02-02-01-16.wav\n",
      "clean_data03-01-03-01-02-01-09.wav\n",
      "clean_data03-01-04-02-02-02-20.wav\n",
      "clean_data03-01-03-02-02-01-13.wav\n",
      "clean_data03-01-04-01-02-01-24.wav\n",
      "clean_data03-01-04-02-02-01-22.wav\n",
      "clean_data03-01-01-01-02-02-19.wav\n",
      "clean_data03-01-02-01-01-02-05.wav\n",
      "clean_data03-01-01-01-01-02-14.wav\n",
      "clean_data03-01-04-01-02-01-13.wav\n",
      "clean_data03-01-02-01-01-02-09.wav\n",
      "clean_data03-01-02-02-02-02-02.wav\n",
      "clean_data03-01-01-01-02-01-18.wav\n",
      "clean_data03-01-04-02-01-01-10.wav\n",
      "clean_data03-01-02-02-01-02-08.wav\n",
      "clean_data03-01-01-01-02-02-16.wav\n",
      "clean_data03-01-01-01-02-01-23.wav\n",
      "clean_data03-01-03-01-01-01-08.wav\n",
      "clean_data03-01-02-01-02-01-22.wav\n",
      "clean_data03-01-04-01-01-01-02.wav\n",
      "clean_data03-01-04-02-02-02-18.wav\n",
      "clean_data03-01-01-01-02-02-17.wav\n",
      "clean_data03-01-01-01-01-01-22.wav\n",
      "clean_data03-01-04-02-01-01-12.wav\n",
      "clean_data03-01-04-01-02-02-18.wav\n",
      "clean_data03-01-03-02-02-01-02.wav\n",
      "clean_data03-01-01-01-01-01-11.wav\n",
      "clean_data03-01-04-01-02-02-12.wav\n",
      "clean_data03-01-04-02-01-02-15.wav\n",
      "clean_data03-01-02-01-01-01-12.wav\n",
      "clean_data03-01-03-01-01-02-16.wav\n",
      "clean_data03-01-01-01-02-01-03.wav\n",
      "clean_data03-01-03-02-01-02-05.wav\n",
      "clean_data03-01-04-02-02-01-24.wav\n",
      "clean_data03-01-04-02-01-02-18.wav\n",
      "clean_data03-01-01-01-01-01-18.wav\n",
      "clean_data03-01-04-02-01-02-19.wav\n",
      "clean_data03-01-03-01-02-02-17.wav\n",
      "clean_data03-01-02-01-02-01-02.wav\n",
      "clean_data03-01-02-02-01-01-20.wav\n",
      "clean_data03-01-01-01-02-01-04.wav\n",
      "clean_data03-01-02-01-01-01-03.wav\n",
      "clean_data03-01-02-01-01-01-14.wav\n",
      "clean_data03-01-02-01-02-02-03.wav\n",
      "clean_data03-01-02-02-01-01-03.wav\n",
      "clean_data03-01-04-02-02-01-14.wav\n",
      "clean_data03-01-04-01-02-01-20.wav\n",
      "clean_data03-01-02-02-02-01-12.wav\n",
      "clean_data03-01-04-01-02-02-11.wav\n",
      "clean_data03-01-01-01-01-02-15.wav\n",
      "clean_data03-01-02-02-01-02-06.wav\n",
      "clean_data03-01-03-01-02-02-10.wav\n",
      "clean_data03-01-04-01-01-02-15.wav\n",
      "clean_data03-01-02-01-01-02-04.wav\n",
      "clean_data03-01-04-01-02-02-07.wav\n",
      "clean_data03-01-03-01-02-02-19.wav\n",
      "clean_data03-01-04-01-02-02-21.wav\n",
      "clean_data03-01-04-01-02-02-09.wav\n",
      "clean_data03-01-04-01-02-01-12.wav\n",
      "clean_data03-01-04-01-01-01-20.wav\n",
      "clean_data03-01-04-01-01-02-12.wav\n",
      "clean_data03-01-04-01-02-02-08.wav\n",
      "clean_data03-01-03-01-02-01-02.wav\n",
      "clean_data03-01-02-02-02-02-13.wav\n",
      "clean_data03-01-03-02-02-02-09.wav\n",
      "clean_data03-01-04-01-01-01-19.wav\n",
      "clean_data03-01-01-01-01-01-07.wav\n",
      "clean_data03-01-01-01-01-01-14.wav\n"
     ]
    }
   ],
   "source": [
    "##split test ddata set\n",
    "##labelling file names to predicted emotion output\n",
    "##also showing the shape tuples(r,c) of our dataSets\n",
    "x_train,x_test,y_trai,y_tes=load_data(test_size=0.25)\n",
    "print(np.shape(x_train),np.shape(x_test),np.shape(y_trai),np.shape(y_tes))\n",
    "y_test_map = np.array(y_tes).T\n",
    "y_test = y_test_map[0]\n",
    "test_filename = y_test_map[1]\n",
    "y_train_map = np.array(y_trai).T\n",
    "y_train = y_train_map[0]\n",
    "train_filename = y_train_map[1]\n",
    "print(np.shape(y_train),np.shape(y_test))\n",
    "print(*test_filename,sep=\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([-5.35849243e+02,  7.55318680e+01, -1.01592093e+01,  5.69860876e-01,\n",
      "       -2.02535820e+01, -1.49986048e+01, -2.56977406e+01, -2.15807724e+01,\n",
      "       -1.25628242e+01, -1.23922682e+01, -8.72327709e+00, -1.44741106e+01,\n",
      "       -5.93426323e+00, -8.21510887e+00, -1.33028297e+01, -5.46756887e+00,\n",
      "       -1.25385141e+01, -6.15823936e+00, -7.92096329e+00, -2.08902076e-01,\n",
      "        6.21816456e-01,  5.57069349e+00,  4.84790993e+00,  1.57187319e+00,\n",
      "       -2.48557329e+00,  3.68227363e+00,  6.15425158e+00,  1.20167704e+01,\n",
      "        1.34859543e+01,  1.55437689e+01,  1.09278345e+01,  8.44227314e+00,\n",
      "        3.42682195e+00, -3.40451550e+00, -7.54415512e+00, -3.84342581e-01,\n",
      "       -1.79084861e+00, -3.03773284e+00, -6.86734962e+00, -4.51440334e+00,\n",
      "        3.53163868e-01,  3.89058441e-01,  4.37834412e-01,  5.12297690e-01,\n",
      "        3.55163455e-01,  3.40410918e-01,  2.87093759e-01,  3.36011887e-01,\n",
      "        5.82483828e-01,  6.12853944e-01,  5.82298100e-01,  4.13455278e-01,\n",
      "        3.94078825e-06,  3.04479613e-06,  5.89574302e-06,  5.77682067e-06,\n",
      "        4.18441095e-06,  8.67073959e-06,  2.35063872e-05,  5.08762943e-03,\n",
      "        4.19161357e-02,  5.25612719e-02,  1.49220014e-02,  4.73984443e-02,\n",
      "        3.25948782e-02,  1.83638511e-03,  4.90803097e-04,  6.08366099e-04,\n",
      "        8.92066676e-03,  1.20698279e-02,  1.81504879e-02,  1.05626993e-02,\n",
      "        4.24170354e-03,  9.86530795e-04,  5.32259094e-03,  1.77321471e-02,\n",
      "        2.08387729e-02,  3.88243161e-02,  7.75116310e-03,  1.18415840e-02,\n",
      "        1.25448564e-02,  2.59061391e-03,  1.83834112e-04,  6.51967493e-05,\n",
      "        4.93585940e-05,  3.08430963e-03,  6.49603596e-03,  2.01617181e-03,\n",
      "        3.90374684e-03,  3.01807257e-03,  2.35985382e-03,  9.49382316e-04,\n",
      "        2.81794695e-04,  1.76812129e-04,  2.44836579e-03,  1.30578608e-03,\n",
      "        8.09758843e-04,  1.75036036e-03,  2.17206054e-03,  1.70099968e-03,\n",
      "        2.96919054e-04,  5.36799605e-04,  1.08026864e-03,  2.21980896e-04,\n",
      "        7.10014894e-04,  9.67874250e-04,  1.77921366e-03,  6.11678232e-04,\n",
      "        5.98416838e-04,  5.23159222e-04,  4.80208866e-04,  4.22834069e-04,\n",
      "        2.71157594e-04,  2.48474505e-04,  5.19448833e-04,  1.77384136e-04,\n",
      "        9.68388631e-05,  1.24774015e-04,  6.52482675e-04,  2.44411378e-04,\n",
      "        5.54054859e-05,  2.91897431e-05,  8.56001061e-05,  1.03012426e-04,\n",
      "        9.81139237e-05,  5.16232503e-05,  7.46025626e-06,  2.65946583e-05,\n",
      "        2.38672092e-05,  2.06624827e-05,  4.14929564e-05,  4.25666221e-05,\n",
      "        3.30549265e-05,  1.98469806e-05,  6.68469875e-05,  5.24657480e-05,\n",
      "        1.74708130e-05,  4.42408018e-05,  2.75371658e-05,  5.18563429e-05,\n",
      "        4.52252680e-05,  5.06501819e-05,  2.55245031e-05,  2.68746280e-05,\n",
      "        2.71950939e-05,  3.65612686e-05,  4.14133647e-05,  4.14727110e-05,\n",
      "        2.88522951e-05,  2.31206795e-05,  1.89052807e-05,  1.68113602e-05,\n",
      "        1.74263278e-05,  3.51967210e-05,  5.28280325e-05,  4.49237305e-05,\n",
      "        4.55127047e-05,  2.26802422e-05,  2.06548502e-05,  1.40356533e-05,\n",
      "        1.05576846e-05,  8.89838429e-06,  7.74001273e-06,  6.63500168e-06,\n",
      "        1.14926106e-05,  8.97674090e-06,  9.85719180e-06,  7.87754379e-06,\n",
      "        1.50829756e-05,  2.24255346e-05,  2.79742399e-05,  4.09944441e-05,\n",
      "        9.24971537e-05,  8.69940268e-05,  1.17660296e-04,  6.45831897e-05,\n",
      "        5.16788823e-05,  3.61844141e-05,  3.08176386e-05,  3.25182082e-05]), array([-5.18235352e+02,  7.54101562e+01, -7.71369457e-01,  7.55515766e+00,\n",
      "       -4.33086681e+00, -1.50651007e+01, -1.02402840e+01, -2.06022758e+01,\n",
      "       -1.09721689e+01, -7.43681240e+00, -1.53435965e+01, -6.62922812e+00,\n",
      "       -8.12557888e+00, -3.46030974e+00, -1.62136078e+01, -2.42522049e+00,\n",
      "       -1.32445841e+01, -9.92685604e+00, -1.09003506e+01, -3.97100401e+00,\n",
      "       -8.82535839e+00,  3.32368469e+00, -1.12112999e+00,  5.56017113e+00,\n",
      "       -1.77592349e+00,  3.19297425e-02, -3.09004021e+00, -1.27757430e-01,\n",
      "       -5.01297045e+00,  1.37162471e+00,  3.42756689e-01,  7.63328457e+00,\n",
      "        4.09512806e+00,  5.63725853e+00, -7.63446271e-01, -2.41093898e+00,\n",
      "        3.37144160e+00,  5.75909758e+00,  3.05789709e+00,  2.78899622e+00,\n",
      "        4.07591283e-01,  4.61977094e-01,  5.21260977e-01,  4.66410697e-01,\n",
      "        4.81728017e-01,  4.80381817e-01,  5.52052736e-01,  5.71574211e-01,\n",
      "        4.83861715e-01,  4.51032102e-01,  4.75479484e-01,  4.32375699e-01,\n",
      "        6.73700197e-06,  1.47308228e-05,  4.02940859e-05,  2.45079300e-05,\n",
      "        3.39651888e-05,  1.13735637e-02,  4.26895134e-02,  5.57521395e-02,\n",
      "        2.18167566e-02,  8.07651319e-03,  4.51254882e-02,  6.27996996e-02,\n",
      "        1.29072238e-02,  5.23143448e-03,  3.97482654e-03,  7.97463395e-03,\n",
      "        1.04751103e-02,  2.16619694e-03,  1.97527814e-03,  6.51647849e-03,\n",
      "        4.29110648e-03,  1.64126214e-02,  1.26355410e-01,  6.32980391e-02,\n",
      "        1.89641304e-02,  4.03363630e-03,  9.55085619e-04,  1.98526867e-03,\n",
      "        3.19204829e-03,  1.47688232e-04,  1.14940638e-04,  7.16622977e-04,\n",
      "        4.23478242e-03,  8.48549139e-03,  1.74768316e-03,  9.79510718e-04,\n",
      "        3.77145770e-04,  2.11518505e-04,  6.51470327e-04,  3.85706400e-04,\n",
      "        1.17728289e-03,  9.14484542e-03,  9.92127322e-03,  1.34878466e-03,\n",
      "        1.72398883e-04,  9.21283572e-05,  9.97363313e-05,  3.64328036e-04,\n",
      "        1.18027267e-03,  2.27952842e-03,  7.36516668e-04,  3.39916005e-04,\n",
      "        8.49682110e-05,  1.26312996e-04,  5.77060331e-04,  2.21335344e-04,\n",
      "        2.72671925e-04,  2.64817936e-04,  2.11097911e-04,  6.34279335e-04,\n",
      "        9.77146701e-05,  1.09605113e-04,  2.61922571e-04,  5.29818644e-04,\n",
      "        1.13751204e-03,  3.53979325e-04,  5.18913512e-05,  1.64602927e-04,\n",
      "        5.69399039e-04,  3.06365546e-04,  1.12120499e-04,  7.99504342e-05,\n",
      "        2.63225684e-05,  4.32438945e-04,  8.13804800e-04,  3.92606125e-05,\n",
      "        6.10030220e-05,  1.33724025e-04,  4.36557457e-04,  2.59671768e-04,\n",
      "        9.73461210e-05,  2.88738229e-04,  1.79106370e-04,  4.75783774e-04,\n",
      "        2.19979251e-04,  1.79793249e-04,  2.26100325e-04,  3.32363794e-04,\n",
      "        1.26156898e-04,  1.31153822e-04,  1.23241218e-04,  7.38412054e-05,\n",
      "        6.70809968e-05,  8.35703759e-05,  8.39897693e-05,  7.79164911e-05,\n",
      "        6.90483721e-05,  1.03282342e-04,  6.85618943e-05,  7.04587001e-05,\n",
      "        1.06406900e-04,  3.55035409e-05,  2.64413575e-05,  2.69159191e-05,\n",
      "        3.41835876e-05,  4.62703065e-05,  5.43715905e-05,  6.82374230e-05,\n",
      "        6.52973904e-05,  4.56962880e-05,  6.78910146e-05,  5.30151810e-05,\n",
      "        5.74544974e-05,  7.03209516e-05,  6.92807225e-05,  3.97053882e-05,\n",
      "        1.50928376e-04,  9.97143070e-05,  8.38640117e-05,  1.52164706e-04,\n",
      "        2.74468912e-04,  2.08515674e-04,  2.07923236e-04,  6.36933764e-05,\n",
      "        3.75671952e-05,  2.39581095e-05,  2.90631160e-05,  8.54949940e-06]))\n"
     ]
    }
   ],
   "source": [
    "#Get Shape of our Training and tesing data\n",
    "print((x_train[0], x_test[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features extracted: 180\n"
     ]
    }
   ],
   "source": [
    "print(f'Features extracted: {x_train.shape[1]}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying Algorithm MLP Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=MLPClassifier(alpha=0.01, batch_size=256, epsilon=1e-08, hidden_layer_sizes=(300,), learning_rate='adaptive', max_iter=500)\n",
    "#DataFlair - Initialize the Multi Layer Perceptron Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dj\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=0.01, batch_size=256, beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(300,), learning_rate='adaptive',\n",
       "       learning_rate_init=0.001, max_iter=500, momentum=0.9,\n",
       "       n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
       "       random_state=None, shuffle=True, solver='adam', tol=0.0001,\n",
       "       validation_fraction=0.1, verbose=False, warm_start=False)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train,y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "Pkl_File = \"Emotion_Voice_Detection_Model.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(Pkl_File,'wb') as file:\n",
    "    pickle.dump(model,file)\n",
    "#storing the model using file handler with 'wb' permission in same dir.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=0.01, batch_size=256, beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(300,), learning_rate='adaptive',\n",
       "       learning_rate_init=0.001, max_iter=500, momentum=0.9,\n",
       "       n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
       "       random_state=None, shuffle=True, solver='adam', tol=0.0001,\n",
       "       validation_fraction=0.1, verbose=False, warm_start=False)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Loading the same model back from file\n",
    "with open(Pkl_File,'rb') as file:\n",
    "    model = pickle.load(file)\n",
    "##display loadead model\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['calm' 'sad' 'sad' 'happy' 'happy' 'calm' 'sad' 'happy' 'calm' 'neutral'\n",
      " 'neutral' 'calm' 'happy' 'neutral' 'happy' 'happy' 'neutral' 'happy'\n",
      " 'calm' 'sad' 'sad' 'neutral' 'calm' 'calm' 'neutral' 'calm' 'sad' 'sad'\n",
      " 'calm' 'happy' 'calm' 'neutral' 'neutral' 'happy' 'sad' 'happy' 'sad'\n",
      " 'sad' 'neutral' 'happy' 'neutral' 'neutral' 'happy' 'neutral' 'calm'\n",
      " 'calm' 'sad' 'happy' 'sad' 'calm' 'sad' 'calm' 'calm' 'calm' 'happy'\n",
      " 'neutral' 'neutral' 'happy' 'happy' 'neutral' 'happy' 'neutral' 'neutral'\n",
      " 'sad' 'sad' 'neutral' 'calm' 'calm' 'sad' 'happy' 'sad' 'sad' 'neutral'\n",
      " 'happy' 'happy' 'neutral' 'calm' 'happy' 'calm' 'happy' 'happy' 'neutral'\n",
      " 'happy' 'neutral' 'neutral' 'happy' 'happy' 'calm' 'happy' 'sad' 'happy'\n",
      " 'calm' 'happy' 'sad' 'happy' 'calm' 'happy' 'calm' 'calm' 'sad' 'sad'\n",
      " 'happy' 'sad' 'happy' 'neutral' 'sad' 'neutral' 'calm' 'neutral' 'sad'\n",
      " 'calm' 'calm' 'neutral' 'happy' 'calm' 'neutral' 'neutral' 'happy' 'calm'\n",
      " 'calm' 'sad' 'neutral' 'sad' 'sad' 'neutral' 'happy' 'neutral' 'sad'\n",
      " 'sad' 'neutral' 'sad' 'neutral' 'happy' 'sad' 'sad' 'neutral' 'neutral'\n",
      " 'happy' 'calm' 'calm' 'neutral' 'calm' 'neutral' 'calm' 'calm' 'sad'\n",
      " 'calm' 'calm' 'neutral' 'neutral' 'calm' 'happy' 'neutral' 'calm' 'happy'\n",
      " 'happy' 'sad' 'calm' 'calm' 'calm' 'neutral' 'calm' 'happy' 'neutral'\n",
      " 'happy' 'sad' 'calm' 'calm']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['calm', 'sad', 'sad', 'happy', 'happy', 'calm', 'sad', 'happy',\n",
       "       'calm', 'neutral', 'neutral', 'calm', 'happy', 'neutral', 'happy',\n",
       "       'happy', 'neutral', 'happy', 'calm', 'sad', 'sad', 'neutral',\n",
       "       'calm', 'calm', 'neutral', 'calm', 'sad', 'sad', 'calm', 'happy',\n",
       "       'calm', 'neutral', 'neutral', 'happy', 'sad', 'happy', 'sad',\n",
       "       'sad', 'neutral', 'happy', 'neutral', 'neutral', 'happy',\n",
       "       'neutral', 'calm', 'calm', 'sad', 'happy', 'sad', 'calm', 'sad',\n",
       "       'calm', 'calm', 'calm', 'happy', 'neutral', 'neutral', 'happy',\n",
       "       'happy', 'neutral', 'happy', 'neutral', 'neutral', 'sad', 'sad',\n",
       "       'neutral', 'calm', 'calm', 'sad', 'happy', 'sad', 'sad', 'neutral',\n",
       "       'happy', 'happy', 'neutral', 'calm', 'happy', 'calm', 'happy',\n",
       "       'happy', 'neutral', 'happy', 'neutral', 'neutral', 'happy',\n",
       "       'happy', 'calm', 'happy', 'sad', 'happy', 'calm', 'happy', 'sad',\n",
       "       'happy', 'calm', 'happy', 'calm', 'calm', 'sad', 'sad', 'happy',\n",
       "       'sad', 'happy', 'neutral', 'sad', 'neutral', 'calm', 'neutral',\n",
       "       'sad', 'calm', 'calm', 'neutral', 'happy', 'calm', 'neutral',\n",
       "       'neutral', 'happy', 'calm', 'calm', 'sad', 'neutral', 'sad', 'sad',\n",
       "       'neutral', 'happy', 'neutral', 'sad', 'sad', 'neutral', 'sad',\n",
       "       'neutral', 'happy', 'sad', 'sad', 'neutral', 'neutral', 'happy',\n",
       "       'calm', 'calm', 'neutral', 'calm', 'neutral', 'calm', 'calm',\n",
       "       'sad', 'calm', 'calm', 'neutral', 'neutral', 'calm', 'happy',\n",
       "       'neutral', 'calm', 'happy', 'happy', 'sad', 'calm', 'calm', 'calm',\n",
       "       'neutral', 'calm', 'happy', 'neutral', 'happy', 'sad', 'calm',\n",
       "       'calm'], dtype='<U7')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(y_pred)\n",
    "single_val = np.array(x_test)\n",
    "np.expand_dims(single_val,0)\n",
    "model.predict(single_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 70.24%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "accuracy=accuracy_score(y_true=y_test, y_pred=y_pred)\n",
    "##checkin out the accuracy\n",
    "print(\"Accuracy: {:.2f}%\".format(accuracy*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary Of Predicted Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sn\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion matrix\n",
      "[[34  1  7  2]\n",
      " [ 1 34  1  4]\n",
      " [ 5  0 24  4]\n",
      " [ 6  8 11 26]]\n",
      "Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        calm       0.74      0.77      0.76        44\n",
      "       happy       0.79      0.85      0.82        40\n",
      "     neutral       0.56      0.73      0.63        33\n",
      "         sad       0.72      0.51      0.60        51\n",
      "\n",
      "   micro avg       0.70      0.70      0.70       168\n",
      "   macro avg       0.70      0.71      0.70       168\n",
      "weighted avg       0.71      0.70      0.70       168\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results = confusion_matrix(y_test,y_pred)\n",
    "print(\"confusion matrix\")\n",
    "'''\n",
    "cm : performance measurement for classification problems\n",
    "true pos : true and right pred   recall = TP/TP+FN\n",
    "true neg : -ve pred and right    prec = TP/TP+FP\n",
    "false pos: wrong pred(error)     acc = 2*recall*prec / recall+prec\n",
    "false neg: is true wrong pred(error)\n",
    "'''\n",
    "print(results)\n",
    "print(\"Report\")\n",
    "report = classification_report(y_test,y_pred)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Storing Predicted File as CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    predictions                          file_names\n",
      "0          calm  clean_data03-01-02-01-01-02-22.wav\n",
      "1           sad  clean_data03-01-04-01-02-02-23.wav\n",
      "2           sad  clean_data03-01-04-02-01-02-05.wav\n",
      "3         happy  clean_data03-01-03-01-01-02-11.wav\n",
      "4         happy  clean_data03-01-03-02-02-02-16.wav\n",
      "5          calm  clean_data03-01-02-02-02-02-21.wav\n",
      "6           sad  clean_data03-01-01-01-02-02-20.wav\n",
      "7         happy  clean_data03-01-03-02-01-02-03.wav\n",
      "8          calm  clean_data03-01-02-02-02-01-11.wav\n",
      "9       neutral  clean_data03-01-01-01-02-01-15.wav\n",
      "10      neutral  clean_data03-01-01-01-01-02-21.wav\n",
      "11         calm  clean_data03-01-02-01-01-01-21.wav\n",
      "12        happy  clean_data03-01-03-02-02-01-16.wav\n",
      "13      neutral  clean_data03-01-02-01-02-01-03.wav\n",
      "14        happy  clean_data03-01-03-01-02-01-14.wav\n",
      "15        happy  clean_data03-01-03-02-01-01-06.wav\n",
      "16      neutral  clean_data03-01-02-01-02-02-06.wav\n",
      "17        happy  clean_data03-01-03-02-02-02-23.wav\n",
      "18         calm  clean_data03-01-02-02-01-01-01.wav\n",
      "19          sad  clean_data03-01-04-01-01-01-03.wav\n",
      "20          sad  clean_data03-01-04-01-02-02-13.wav\n",
      "21      neutral  clean_data03-01-01-01-01-02-17.wav\n",
      "22         calm  clean_data03-01-02-02-02-02-08.wav\n",
      "23         calm  clean_data03-01-02-01-01-01-11.wav\n",
      "24      neutral  clean_data03-01-01-01-01-02-16.wav\n",
      "25         calm  clean_data03-01-02-02-01-01-04.wav\n",
      "26          sad  clean_data03-01-04-02-02-02-24.wav\n",
      "27          sad  clean_data03-01-03-02-01-02-11.wav\n",
      "28         calm  clean_data03-01-01-01-01-02-07.wav\n",
      "29        happy  clean_data03-01-03-01-01-02-18.wav\n",
      "..          ...                                 ...\n",
      "138        calm  clean_data03-01-02-01-02-01-02.wav\n",
      "139        calm  clean_data03-01-02-02-01-01-20.wav\n",
      "140     neutral  clean_data03-01-01-01-02-01-04.wav\n",
      "141        calm  clean_data03-01-02-01-01-01-03.wav\n",
      "142     neutral  clean_data03-01-02-01-01-01-14.wav\n",
      "143        calm  clean_data03-01-02-01-02-02-03.wav\n",
      "144        calm  clean_data03-01-02-02-01-01-03.wav\n",
      "145         sad  clean_data03-01-04-02-02-01-14.wav\n",
      "146        calm  clean_data03-01-04-01-02-01-20.wav\n",
      "147        calm  clean_data03-01-02-02-02-01-12.wav\n",
      "148     neutral  clean_data03-01-04-01-02-02-11.wav\n",
      "149     neutral  clean_data03-01-01-01-01-02-15.wav\n",
      "150        calm  clean_data03-01-02-02-01-02-06.wav\n",
      "151       happy  clean_data03-01-03-01-02-02-10.wav\n",
      "152     neutral  clean_data03-01-04-01-01-02-15.wav\n",
      "153        calm  clean_data03-01-02-01-01-02-04.wav\n",
      "154       happy  clean_data03-01-04-01-02-02-07.wav\n",
      "155       happy  clean_data03-01-03-01-02-02-19.wav\n",
      "156         sad  clean_data03-01-04-01-02-02-21.wav\n",
      "157        calm  clean_data03-01-04-01-02-02-09.wav\n",
      "158        calm  clean_data03-01-04-01-02-01-12.wav\n",
      "159        calm  clean_data03-01-04-01-01-01-20.wav\n",
      "160     neutral  clean_data03-01-04-01-01-02-12.wav\n",
      "161        calm  clean_data03-01-04-01-02-02-08.wav\n",
      "162       happy  clean_data03-01-03-01-02-01-02.wav\n",
      "163     neutral  clean_data03-01-02-02-02-02-13.wav\n",
      "164       happy  clean_data03-01-03-02-02-02-09.wav\n",
      "165         sad  clean_data03-01-04-01-01-01-19.wav\n",
      "166        calm  clean_data03-01-01-01-01-01-07.wav\n",
      "167        calm  clean_data03-01-01-01-01-01-14.wav\n",
      "\n",
      "[168 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "dataF = pd.DataFrame(y_pred, columns = ['predictions'])\n",
    "dataF['file_names'] = test_filename #printing with their previous label names\n",
    "dataF['file_names'] = test_filename #printing with their previous label names\n",
    "print(dataF)\n",
    "dataF.to_csv('output_prediction.csv') \n",
    "#we get ith wav file , got predicted emotion and so on in a readable format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Specify PyAudio and Sampling bit-rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk = 1024  # Record in chunks of 1024 samples\n",
    "sample_format = pyaudio.paInt16  # 16 bits per sample\n",
    "channels = 2\n",
    "fs = 44100  # Record at 44100 samples per second\n",
    "seconds = 4\n",
    "filename = \"getC12345.wav\"\n",
    "\n",
    "p = pyaudio.PyAudio()  # Create an interface to PortAudio\n",
    "print('Recording')\n",
    "\n",
    "stream = p.open(format=sample_format,\n",
    "                channels=channels,\n",
    "                rate=fs,\n",
    "                frames_per_buffer=chunk,\n",
    "                input=True)\n",
    "\n",
    "frames = []  # Initialize array to store frames\n",
    "\n",
    "# Store data in chunks for 3 seconds\n",
    "for i in range(0, int(fs / chunk * seconds)):\n",
    "    data = stream.read(chunk)\n",
    "    frames.append(data)\n",
    "\n",
    "# Stop and close the stream \n",
    "stream.stop_stream()\n",
    "stream.close()\n",
    "# Terminate the PortAudio interface\n",
    "p.terminate()\n",
    "\n",
    "print('Finished recording')\n",
    "wf = wave.open(filename, 'wb')\n",
    "wf.setnchannels(channels)\n",
    "wf.setsampwidth(p.get_sample_size(sample_format))\n",
    "wf.setframerate(fs)\n",
    "wf.writeframes(b''.join(frames))\n",
    "wf.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Recorded Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using for testing recorded voice.\n",
    "playsound(filename)\n",
    "file_name = filename # so that we edit a copy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Display WavePlot For Sample Recorded Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "from librosa import display\n",
    "data,sampling_rate = librosa.load(file_name)\n",
    "plt.figure(figsize=(15,5))\n",
    "librosa.display.waveplot(data,sr=sampling_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans = []\n",
    "newA=\"test\"\n",
    "from pydub import AudioSegment\n",
    "sound = AudioSegment.from_wav(file_name)\n",
    "sound = sound.set_channels(1)\n",
    "sound.export(newA, format=\"wav\")\n",
    "##there are 2 types of audio's, mono and stereo.\n",
    "##extract feature expects a mono. thus its need to be converted using audio Segment.\n",
    "new_feature= extract_feature(newA,mfcc=True,chroma=True,mel=True)\n",
    "ans.append(new_feature)\n",
    "ans = np.array(ans)\n",
    "\n",
    "model.predict(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
